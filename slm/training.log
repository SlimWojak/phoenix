============================================================
HARDWARE CHECK
============================================================
MLX device: Device(gpu, 0)
Metal available: True
MLX computation test: PASS
============================================================

============================================================
STARTING TRAINING
============================================================

Loading model: Qwen/Qwen2.5-1.5B-Instruct
This may take a few minutes on first run (downloading)...
Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 122333.87it/s]
mx.metal.device_info is deprecated and will be removed in a future version. Use mx.device_info instead.
Model loaded: Model
Tokenizer: TokenizerWrapper

Preparing training data...
Train: slm/training/train.jsonl
Val: slm/training/valid.jsonl
Train examples: 800
Val examples: 100

LoRA config: {'rank': 32, 'alpha': 64, 'dropout': 0.05, 'scale': 2.0}
Training args: batch=4, lr=0.0002

Applying LoRA layers...
LoRA trainable parameters: 10,551,296 / 233,375,232 (4.52%)

Loading datasets...
Train dataset: 800 examples
Val dataset: 100 examples

Starting training loop...
Starting training..., iters: 600
Calculating loss...:   0%|          | 0/10 [00:00<?, ?it/s]Calculating loss...:  10%|█         | 1/10 [00:00<00:01,  4.55it/s]Calculating loss...:  20%|██        | 2/10 [00:00<00:02,  2.77it/s]Calculating loss...:  30%|███       | 3/10 [00:00<00:02,  3.20it/s]Calculating loss...:  40%|████      | 4/10 [00:01<00:01,  3.45it/s]Calculating loss...:  50%|█████     | 5/10 [00:01<00:01,  3.61it/s]Calculating loss...:  60%|██████    | 6/10 [00:01<00:01,  3.83it/s]Calculating loss...:  70%|███████   | 7/10 [00:01<00:00,  3.85it/s]Calculating loss...:  80%|████████  | 8/10 [00:02<00:00,  3.88it/s]Calculating loss...:  90%|█████████ | 9/10 [00:02<00:00,  4.17it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.38it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  3.86it/s]
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 302 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 263 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 1: Val loss 3.876, Val took 2.591s
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 296 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 10: Train loss 5.701, Learning Rate 2.000e-04, It/sec 2.064, Tokens/sec 1378.233, Trained Tokens 6676, Peak mem 8.715 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 299 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 296 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 296 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 20: Train loss 5.931, Learning Rate 2.000e-04, It/sec 2.406, Tokens/sec 1548.562, Trained Tokens 13111, Peak mem 8.715 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 295 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 295 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 30: Train loss 9.333, Learning Rate 2.000e-04, It/sec 2.649, Tokens/sec 1598.159, Trained Tokens 19145, Peak mem 8.715 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 284 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 284 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 40: Train loss 4.570, Learning Rate 2.000e-04, It/sec 2.537, Tokens/sec 1542.271, Trained Tokens 25225, Peak mem 8.715 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 289 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 294 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 284 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
Calculating loss...:   0%|          | 0/10 [00:00<?, ?it/s]Calculating loss...:  10%|█         | 1/10 [00:00<00:01,  4.83it/s]Calculating loss...:  20%|██        | 2/10 [00:00<00:01,  4.20it/s]Calculating loss...:  30%|███       | 3/10 [00:00<00:01,  4.51it/s]Calculating loss...:  40%|████      | 4/10 [00:00<00:01,  4.90it/s]Calculating loss...:  50%|█████     | 5/10 [00:01<00:01,  4.48it/s]Calculating loss...:  60%|██████    | 6/10 [00:01<00:00,  4.25it/s]Calculating loss...:  70%|███████   | 7/10 [00:01<00:00,  4.12it/s]Calculating loss...:  80%|████████  | 8/10 [00:01<00:00,  4.03it/s]Calculating loss...:  90%|█████████ | 9/10 [00:02<00:00,  4.13it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.51it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.38it/s]
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 302 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 266 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 293 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 263 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 50: Val loss 2.447, Val took 2.290s
Iter 50: Train loss 3.239, Learning Rate 2.000e-04, It/sec 2.411, Tokens/sec 1586.366, Trained Tokens 31806, Peak mem 8.842 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 60: Train loss 2.241, Learning Rate 2.000e-04, It/sec 2.420, Tokens/sec 1524.152, Trained Tokens 38103, Peak mem 8.842 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 299 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 295 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 301 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 296 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 70: Train loss 21.821, Learning Rate 2.000e-04, It/sec 2.250, Tokens/sec 1513.838, Trained Tokens 44831, Peak mem 8.842 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 289 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 289 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 80: Train loss 15.122, Learning Rate 2.000e-04, It/sec 2.430, Tokens/sec 1635.042, Trained Tokens 51559, Peak mem 8.842 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 303 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 295 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 90: Train loss 3.574, Learning Rate 2.000e-04, It/sec 2.408, Tokens/sec 1627.527, Trained Tokens 58319, Peak mem 8.842 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 294 will be truncated to 256. Consider pre-splitting your data to save memory.
Calculating loss...:   0%|          | 0/10 [00:00<?, ?it/s]Calculating loss...:  10%|█         | 1/10 [00:00<00:02,  4.33it/s]Calculating loss...:  20%|██        | 2/10 [00:00<00:01,  5.07it/s]Calculating loss...:  30%|███       | 3/10 [00:00<00:01,  5.35it/s]Calculating loss...:  40%|████      | 4/10 [00:00<00:01,  4.65it/s]Calculating loss...:  50%|█████     | 5/10 [00:01<00:01,  4.35it/s]Calculating loss...:  60%|██████    | 6/10 [00:01<00:00,  4.18it/s]Calculating loss...:  70%|███████   | 7/10 [00:01<00:00,  4.40it/s]Calculating loss...:  80%|████████  | 8/10 [00:01<00:00,  4.57it/s]Calculating loss...:  90%|█████████ | 9/10 [00:02<00:00,  4.33it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.50it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.52it/s]
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 281 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 266 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 100: Val loss 2.794, Val took 2.217s
Iter 100: Train loss 2.787, Learning Rate 2.000e-04, It/sec 2.805, Tokens/sec 1625.108, Trained Tokens 64112, Peak mem 8.842 GB
Iter 100: Saved adapter weights to slm/models/phoenix-guard/adapters.safetensors and slm/models/phoenix-guard/0000100_adapters.safetensors.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 110: Train loss 2.278, Learning Rate 2.000e-04, It/sec 2.347, Tokens/sec 1577.192, Trained Tokens 70832, Peak mem 9.506 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 289 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 284 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 283 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 120: Train loss 1.826, Learning Rate 2.000e-04, It/sec 2.289, Tokens/sec 1606.915, Trained Tokens 77852, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 294 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 292 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 130: Train loss 1.421, Learning Rate 2.000e-04, It/sec 2.405, Tokens/sec 1581.077, Trained Tokens 84426, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 289 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 295 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 140: Train loss 1.192, Learning Rate 2.000e-04, It/sec 2.376, Tokens/sec 1538.745, Trained Tokens 90901, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 301 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 299 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 292 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 267 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 284 will be truncated to 256. Consider pre-splitting your data to save memory.
Calculating loss...:   0%|          | 0/10 [00:00<?, ?it/s]Calculating loss...:  10%|█         | 1/10 [00:00<00:02,  3.89it/s]Calculating loss...:  20%|██        | 2/10 [00:00<00:01,  4.45it/s]Calculating loss...:  30%|███       | 3/10 [00:00<00:01,  4.93it/s]Calculating loss...:  40%|████      | 4/10 [00:00<00:01,  4.45it/s]Calculating loss...:  50%|█████     | 5/10 [00:01<00:01,  4.61it/s]Calculating loss...:  60%|██████    | 6/10 [00:01<00:00,  4.34it/s]Calculating loss...:  70%|███████   | 7/10 [00:01<00:00,  4.72it/s]Calculating loss...:  80%|████████  | 8/10 [00:01<00:00,  4.99it/s]Calculating loss...:  90%|█████████ | 9/10 [00:01<00:00,  4.58it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.33it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.50it/s]
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 302 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 283 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 293 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 296 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 150: Val loss 0.978, Val took 2.223s
Iter 150: Train loss 1.066, Learning Rate 2.000e-04, It/sec 2.411, Tokens/sec 1619.089, Trained Tokens 97617, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 299 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 160: Train loss 0.887, Learning Rate 2.000e-04, It/sec 2.551, Tokens/sec 1606.110, Trained Tokens 103914, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 295 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 294 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 267 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 170: Train loss 0.801, Learning Rate 2.000e-04, It/sec 2.483, Tokens/sec 1599.701, Trained Tokens 110356, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 268 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 292 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 289 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 180: Train loss 0.627, Learning Rate 2.000e-04, It/sec 2.489, Tokens/sec 1643.718, Trained Tokens 116959, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 293 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 295 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 190: Train loss 0.583, Learning Rate 2.000e-04, It/sec 2.467, Tokens/sec 1569.779, Trained Tokens 123323, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 290 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 293 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 294 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
Calculating loss...:   0%|          | 0/10 [00:00<?, ?it/s]Calculating loss...:  10%|█         | 1/10 [00:00<00:01,  5.72it/s]Calculating loss...:  20%|██        | 2/10 [00:00<00:01,  4.68it/s]Calculating loss...:  30%|███       | 3/10 [00:00<00:01,  4.28it/s]Calculating loss...:  40%|████      | 4/10 [00:00<00:01,  4.10it/s]Calculating loss...:  50%|█████     | 5/10 [00:01<00:01,  4.58it/s]Calculating loss...:  60%|██████    | 6/10 [00:01<00:00,  4.31it/s]Calculating loss...:  70%|███████   | 7/10 [00:01<00:00,  4.16it/s]Calculating loss...:  80%|████████  | 8/10 [00:01<00:00,  4.56it/s]Calculating loss...:  90%|█████████ | 9/10 [00:01<00:00,  4.88it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.51it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.49it/s]
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 266 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 263 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 283 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 293 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 200: Val loss 0.573, Val took 2.232s
Iter 200: Train loss 0.570, Learning Rate 2.000e-04, It/sec 2.277, Tokens/sec 1510.996, Trained Tokens 129958, Peak mem 9.633 GB
Iter 200: Saved adapter weights to slm/models/phoenix-guard/adapters.safetensors and slm/models/phoenix-guard/0000200_adapters.safetensors.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 292 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 210: Train loss 0.504, Learning Rate 2.000e-04, It/sec 2.458, Tokens/sec 1579.251, Trained Tokens 136382, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 294 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 220: Train loss 0.442, Learning Rate 2.000e-04, It/sec 2.579, Tokens/sec 1592.447, Trained Tokens 142557, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 283 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 295 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 230: Train loss 0.404, Learning Rate 2.000e-04, It/sec 2.434, Tokens/sec 1627.605, Trained Tokens 149244, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 284 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 295 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 240: Train loss 0.369, Learning Rate 2.000e-04, It/sec 2.332, Tokens/sec 1520.147, Trained Tokens 155763, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 268 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 303 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 289 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 293 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 284 will be truncated to 256. Consider pre-splitting your data to save memory.
Calculating loss...:   0%|          | 0/10 [00:00<?, ?it/s]Calculating loss...:  10%|█         | 1/10 [00:00<00:02,  3.89it/s]Calculating loss...:  20%|██        | 2/10 [00:00<00:01,  4.14it/s]Calculating loss...:  30%|███       | 3/10 [00:00<00:01,  4.46it/s]Calculating loss...:  40%|████      | 4/10 [00:00<00:01,  4.20it/s]Calculating loss...:  50%|█████     | 5/10 [00:01<00:01,  4.43it/s]Calculating loss...:  60%|██████    | 6/10 [00:01<00:00,  4.21it/s]Calculating loss...:  70%|███████   | 7/10 [00:01<00:00,  4.10it/s]Calculating loss...:  80%|████████  | 8/10 [00:01<00:00,  4.03it/s]Calculating loss...:  90%|█████████ | 9/10 [00:02<00:00,  4.44it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.77it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.39it/s]
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 281 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 266 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 266 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 250: Val loss 0.324, Val took 2.281s
Iter 250: Train loss 0.312, Learning Rate 2.000e-04, It/sec 2.326, Tokens/sec 1602.971, Trained Tokens 162654, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 301 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 295 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 294 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 299 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 260: Train loss 0.382, Learning Rate 2.000e-04, It/sec 2.395, Tokens/sec 1570.504, Trained Tokens 169211, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 294 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 296 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 289 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 292 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 270: Train loss 0.333, Learning Rate 2.000e-04, It/sec 2.496, Tokens/sec 1610.929, Trained Tokens 175666, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 299 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 280: Train loss 0.285, Learning Rate 2.000e-04, It/sec 2.343, Tokens/sec 1635.527, Trained Tokens 182646, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 299 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 290: Train loss 0.349, Learning Rate 2.000e-04, It/sec 2.585, Tokens/sec 1605.788, Trained Tokens 188858, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
Calculating loss...:   0%|          | 0/10 [00:00<?, ?it/s]Calculating loss...:  10%|█         | 1/10 [00:00<00:02,  3.89it/s]Calculating loss...:  20%|██        | 2/10 [00:00<00:01,  4.41it/s]Calculating loss...:  30%|███       | 3/10 [00:00<00:01,  4.64it/s]Calculating loss...:  40%|████      | 4/10 [00:00<00:01,  4.31it/s]Calculating loss...:  50%|█████     | 5/10 [00:01<00:01,  4.15it/s]Calculating loss...:  60%|██████    | 6/10 [00:01<00:00,  4.38it/s]Calculating loss...:  70%|███████   | 7/10 [00:01<00:00,  4.20it/s]Calculating loss...:  80%|████████  | 8/10 [00:01<00:00,  4.41it/s]Calculating loss...:  90%|█████████ | 9/10 [00:02<00:00,  4.70it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.59it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.43it/s]
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 266 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 283 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 266 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 300: Val loss 0.327, Val took 2.259s
Iter 300: Train loss 0.314, Learning Rate 2.000e-04, It/sec 2.464, Tokens/sec 1563.189, Trained Tokens 195202, Peak mem 9.633 GB
Iter 300: Saved adapter weights to slm/models/phoenix-guard/adapters.safetensors and slm/models/phoenix-guard/0000300_adapters.safetensors.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 289 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 296 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 310: Train loss 0.245, Learning Rate 2.000e-04, It/sec 2.440, Tokens/sec 1555.269, Trained Tokens 201575, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 301 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 294 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 267 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 320: Train loss 0.302, Learning Rate 2.000e-04, It/sec 2.269, Tokens/sec 1551.064, Trained Tokens 208411, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 284 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 289 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 330: Train loss 0.347, Learning Rate 2.000e-04, It/sec 2.489, Tokens/sec 1602.059, Trained Tokens 214847, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 340: Train loss 0.243, Learning Rate 2.000e-04, It/sec 2.681, Tokens/sec 1643.330, Trained Tokens 220977, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 284 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 292 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 267 will be truncated to 256. Consider pre-splitting your data to save memory.
Calculating loss...:   0%|          | 0/10 [00:00<?, ?it/s]Calculating loss...:  10%|█         | 1/10 [00:00<00:02,  3.87it/s]Calculating loss...:  20%|██        | 2/10 [00:00<00:01,  4.76it/s]Calculating loss...:  30%|███       | 3/10 [00:00<00:01,  4.54it/s]Calculating loss...:  40%|████      | 4/10 [00:00<00:01,  4.23it/s]Calculating loss...:  50%|█████     | 5/10 [00:01<00:01,  4.26it/s]Calculating loss...:  60%|██████    | 6/10 [00:01<00:00,  4.46it/s]Calculating loss...:  70%|███████   | 7/10 [00:01<00:00,  4.57it/s]Calculating loss...:  80%|████████  | 8/10 [00:01<00:00,  4.30it/s]Calculating loss...:  90%|█████████ | 9/10 [00:02<00:00,  4.66it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.37it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.41it/s]
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 266 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 281 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 302 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 293 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 350: Val loss 0.283, Val took 2.280s
Iter 350: Train loss 0.279, Learning Rate 2.000e-04, It/sec 2.404, Tokens/sec 1571.024, Trained Tokens 227511, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 299 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 293 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 296 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 289 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 360: Train loss 0.270, Learning Rate 2.000e-04, It/sec 2.279, Tokens/sec 1559.931, Trained Tokens 234357, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 295 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 295 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 370: Train loss 0.268, Learning Rate 2.000e-04, It/sec 2.315, Tokens/sec 1506.254, Trained Tokens 240863, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 290 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 294 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 295 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 380: Train loss 0.246, Learning Rate 2.000e-04, It/sec 2.421, Tokens/sec 1532.210, Trained Tokens 247192, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 295 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 289 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 284 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 390: Train loss 0.241, Learning Rate 2.000e-04, It/sec 2.557, Tokens/sec 1547.670, Trained Tokens 253245, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 296 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
Calculating loss...:   0%|          | 0/10 [00:00<?, ?it/s]Calculating loss...:  10%|█         | 1/10 [00:00<00:02,  3.86it/s]Calculating loss...:  20%|██        | 2/10 [00:00<00:02,  3.85it/s]Calculating loss...:  30%|███       | 3/10 [00:00<00:01,  3.83it/s]Calculating loss...:  40%|████      | 4/10 [00:01<00:01,  3.84it/s]Calculating loss...:  50%|█████     | 5/10 [00:01<00:01,  4.15it/s]Calculating loss...:  60%|██████    | 6/10 [00:01<00:00,  4.04it/s]Calculating loss...:  70%|███████   | 7/10 [00:01<00:00,  3.98it/s]Calculating loss...:  80%|████████  | 8/10 [00:01<00:00,  4.23it/s]Calculating loss...:  90%|█████████ | 9/10 [00:02<00:00,  4.10it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.02it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.01it/s]
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 263 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 302 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 266 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 293 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 400: Val loss 0.212, Val took 2.501s
Iter 400: Train loss 0.253, Learning Rate 2.000e-04, It/sec 2.264, Tokens/sec 1510.272, Trained Tokens 259916, Peak mem 9.633 GB
Iter 400: Saved adapter weights to slm/models/phoenix-guard/adapters.safetensors and slm/models/phoenix-guard/0000400_adapters.safetensors.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 295 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 295 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 293 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 299 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 410: Train loss 0.195, Learning Rate 2.000e-04, It/sec 2.439, Tokens/sec 1609.974, Trained Tokens 266518, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 289 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 295 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 294 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 420: Train loss 0.248, Learning Rate 2.000e-04, It/sec 2.323, Tokens/sec 1631.374, Trained Tokens 273541, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 296 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 430: Train loss 0.213, Learning Rate 2.000e-04, It/sec 2.595, Tokens/sec 1554.408, Trained Tokens 279530, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 293 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 303 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 299 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 284 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 440: Train loss 0.216, Learning Rate 2.000e-04, It/sec 2.402, Tokens/sec 1540.863, Trained Tokens 285944, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 289 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 283 will be truncated to 256. Consider pre-splitting your data to save memory.
Calculating loss...:   0%|          | 0/10 [00:00<?, ?it/s]Calculating loss...:  10%|█         | 1/10 [00:00<00:01,  5.77it/s]Calculating loss...:  20%|██        | 2/10 [00:00<00:01,  5.27it/s]Calculating loss...:  30%|███       | 3/10 [00:00<00:01,  4.78it/s]Calculating loss...:  40%|████      | 4/10 [00:00<00:01,  4.35it/s]Calculating loss...:  50%|█████     | 5/10 [00:01<00:01,  4.16it/s]Calculating loss...:  60%|██████    | 6/10 [00:01<00:00,  4.58it/s]Calculating loss...:  70%|███████   | 7/10 [00:01<00:00,  4.68it/s]Calculating loss...:  80%|████████  | 8/10 [00:01<00:00,  4.36it/s]Calculating loss...:  90%|█████████ | 9/10 [00:02<00:00,  4.21it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.60it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.55it/s]
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 296 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 266 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 293 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 450: Val loss 0.213, Val took 2.200s
Iter 450: Train loss 0.215, Learning Rate 2.000e-04, It/sec 2.550, Tokens/sec 1652.255, Trained Tokens 292423, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 460: Train loss 0.219, Learning Rate 2.000e-04, It/sec 2.518, Tokens/sec 1594.088, Trained Tokens 298755, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 296 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 294 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 470: Train loss 0.298, Learning Rate 2.000e-04, It/sec 2.389, Tokens/sec 1540.283, Trained Tokens 305203, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 267 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 480: Train loss 0.205, Learning Rate 2.000e-04, It/sec 2.398, Tokens/sec 1494.763, Trained Tokens 311436, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 289 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 295 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 296 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 490: Train loss 0.198, Learning Rate 2.000e-04, It/sec 2.331, Tokens/sec 1610.853, Trained Tokens 318346, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
Calculating loss...:   0%|          | 0/10 [00:00<?, ?it/s]Calculating loss...:  10%|█         | 1/10 [00:00<00:02,  3.88it/s]Calculating loss...:  20%|██        | 2/10 [00:00<00:01,  4.13it/s]Calculating loss...:  30%|███       | 3/10 [00:00<00:01,  4.22it/s]Calculating loss...:  40%|████      | 4/10 [00:00<00:01,  4.08it/s]Calculating loss...:  50%|█████     | 5/10 [00:01<00:01,  4.00it/s]Calculating loss...:  60%|██████    | 6/10 [00:01<00:01,  3.96it/s]Calculating loss...:  70%|███████   | 7/10 [00:01<00:00,  4.22it/s]Calculating loss...:  80%|████████  | 8/10 [00:01<00:00,  4.11it/s]Calculating loss...:  90%|█████████ | 9/10 [00:02<00:00,  4.33it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.69it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.29it/s]
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 281 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 263 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 266 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 302 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 500: Val loss 0.236, Val took 2.336s
Iter 500: Train loss 0.279, Learning Rate 2.000e-04, It/sec 2.467, Tokens/sec 1598.850, Trained Tokens 324827, Peak mem 9.633 GB
Iter 500: Saved adapter weights to slm/models/phoenix-guard/adapters.safetensors and slm/models/phoenix-guard/0000500_adapters.safetensors.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 289 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 284 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 295 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 510: Train loss 0.209, Learning Rate 2.000e-04, It/sec 2.430, Tokens/sec 1591.351, Trained Tokens 331375, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 284 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 299 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 294 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 520: Train loss 0.202, Learning Rate 2.000e-04, It/sec 2.334, Tokens/sec 1650.317, Trained Tokens 338446, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 292 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 295 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 295 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 299 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 530: Train loss 0.215, Learning Rate 2.000e-04, It/sec 2.214, Tokens/sec 1494.832, Trained Tokens 345198, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 289 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 267 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 540: Train loss 0.198, Learning Rate 2.000e-04, It/sec 2.370, Tokens/sec 1567.543, Trained Tokens 351812, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 301 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 292 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 301 will be truncated to 256. Consider pre-splitting your data to save memory.
Calculating loss...:   0%|          | 0/10 [00:00<?, ?it/s]Calculating loss...:  10%|█         | 1/10 [00:00<00:02,  3.87it/s]Calculating loss...:  20%|██        | 2/10 [00:00<00:01,  4.38it/s]Calculating loss...:  30%|███       | 3/10 [00:00<00:01,  4.12it/s]Calculating loss...:  40%|████      | 4/10 [00:00<00:01,  4.36it/s]Calculating loss...:  50%|█████     | 5/10 [00:01<00:01,  4.75it/s]Calculating loss...:  60%|██████    | 6/10 [00:01<00:00,  5.00it/s]Calculating loss...:  70%|███████   | 7/10 [00:01<00:00,  4.95it/s]Calculating loss...:  80%|████████  | 8/10 [00:01<00:00,  4.53it/s]Calculating loss...:  90%|█████████ | 9/10 [00:01<00:00,  4.82it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.46it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.55it/s]
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 296 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 263 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 550: Val loss 0.219, Val took 2.204s
Iter 550: Train loss 0.243, Learning Rate 2.000e-04, It/sec 2.371, Tokens/sec 1563.400, Trained Tokens 358405, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 284 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 268 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 284 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 560: Train loss 0.222, Learning Rate 2.000e-04, It/sec 2.641, Tokens/sec 1569.668, Trained Tokens 364348, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 290 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 570: Train loss 0.251, Learning Rate 2.000e-04, It/sec 2.426, Tokens/sec 1564.397, Trained Tokens 370796, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 287 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 580: Train loss 0.214, Learning Rate 2.000e-04, It/sec 2.487, Tokens/sec 1603.569, Trained Tokens 377244, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 294 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 285 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 294 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 590: Train loss 0.251, Learning Rate 2.000e-04, It/sec 2.402, Tokens/sec 1504.934, Trained Tokens 383510, Peak mem 9.633 GB
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 292 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 296 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 297 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 289 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 286 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 288 will be truncated to 256. Consider pre-splitting your data to save memory.
Calculating loss...:   0%|          | 0/10 [00:00<?, ?it/s]Calculating loss...:  10%|█         | 1/10 [00:00<00:02,  3.86it/s]Calculating loss...:  20%|██        | 2/10 [00:00<00:01,  4.75it/s]Calculating loss...:  30%|███       | 3/10 [00:00<00:01,  4.75it/s]Calculating loss...:  40%|████      | 4/10 [00:00<00:01,  4.33it/s]Calculating loss...:  50%|█████     | 5/10 [00:01<00:01,  4.51it/s]Calculating loss...:  60%|██████    | 6/10 [00:01<00:00,  4.24it/s]Calculating loss...:  70%|███████   | 7/10 [00:01<00:00,  4.43it/s]Calculating loss...:  80%|████████  | 8/10 [00:01<00:00,  4.38it/s]Calculating loss...:  90%|█████████ | 9/10 [00:02<00:00,  4.20it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.08it/s]Calculating loss...: 100%|██████████| 10/10 [00:02<00:00,  4.28it/s]
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 296 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 300 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 266 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 293 will be truncated to 256. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 256 tokens. The longest sentence 298 will be truncated to 256. Consider pre-splitting your data to save memory.
Iter 600: Val loss 0.210, Val took 2.339s
Iter 600: Train loss 0.199, Learning Rate 2.000e-04, It/sec 2.332, Tokens/sec 1483.983, Trained Tokens 389874, Peak mem 9.633 GB
Traceback (most recent call last):
  File "/Users/echopeso/phoenix/slm/train_slm.py", line 493, in main
    success = run_inference_test(config, model_path)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/echopeso/phoenix/slm/train_slm.py", line 289, in run_inference_test
    model, tokenizer = load(str(model_path))
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/echopeso/phoenix/.venv/lib/python3.12/site-packages/mlx_lm/utils.py", line 463, in load
    model, config = load_model(model_path, lazy, model_config=model_config)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/echopeso/phoenix/.venv/lib/python3.12/site-packages/mlx_lm/utils.py", line 299, in load_model
    config = load_config(model_path)
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/echopeso/phoenix/.venv/lib/python3.12/site-packages/mlx_lm/utils.py", line 251, in load_config
    with open(model_path / "config.json", "r") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'slm/models/phoenix-guard/config.json'
Iter 600: Saved adapter weights to slm/models/phoenix-guard/adapters.safetensors and slm/models/phoenix-guard/0000600_adapters.safetensors.
Saved final weights to slm/models/phoenix-guard/adapters.safetensors.

Training complete in 4.7 minutes

Model saved to slm/models/phoenix-guard/adapters.safetensors

============================================================
INFERENCE TEST
============================================================

ERROR: [Errno 2] No such file or directory: 'slm/models/phoenix-guard/config.json'
